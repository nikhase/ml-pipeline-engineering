{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa4ca38-cc2a-4d7b-95cf-d14abcd13d33",
   "metadata": {},
   "source": [
    "# Simple and Robust ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed84ba3-570e-4b65-b12d-3c9550ae0e96",
   "metadata": {},
   "source": [
    "With this notebook, we want to work out the fundamentals of a robust ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e06fda-3182-4d9c-96e4-7d32c678ed86",
   "metadata": {},
   "source": [
    "Most ML pipelines (or data pipelines) follow a similar structure. The following graph can be seen as a minimal ML pipeline. The `preparation`, `training` and `validation` parts of the pipeline can be seen as `ML (related) code`.\n",
    "\n",
    "![How a data pipeline looks like](https://cloud.google.com/architecture/images/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning-2-manual-ml.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3450010-6e6f-4f53-9662-ecd8a2795669",
   "metadata": {},
   "source": [
    "There is research about the fact that ML Code is just a small part when doing Machine Learning:\n",
    "\n",
    "![ML Code is just a small part when doing Machine Learning](https://cloud.google.com/architecture/images/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning-1-elements-of-ml.png)\n",
    "\n",
    "Original Source: *Sculley, David, et al. \"Hidden technical debt in machine learning systems.\" Advances in neural information processing systems 28 (2015): 2503-2511.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57c099-f15a-4374-870a-e561fa7e9d6f",
   "metadata": {},
   "source": [
    "**In this notebook, we want to show by example how to transform prototype ML code which requires manual execution into a robust, scalable ML pipeline.**\n",
    "\n",
    "It is NOT about tooling, it is about understanding the fundamental principles of a robust ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806564e9-5dd7-492e-9d88-fa5af35fd8f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e243a9a1-b5ee-4cdc-ae84-75273f0f54f0",
   "metadata": {},
   "source": [
    "This is your small pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a873e-b3cc-4865-a33c-066743631582",
   "metadata": {},
   "source": [
    "## The Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c5861-661a-45b2-bfbe-28985206a0e9",
   "metadata": {},
   "source": [
    "We build a classifier for Iris (german: \"Schwertlilie\").\n",
    "For features, we use speal width and length.\n",
    "\n",
    "![Irises with sepal and petal](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)\n",
    "\n",
    "Source: [Machine Learning in R for beginners](https://www.datacamp.com/community/tutorials/machine-learning-in-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e688fa-f505-4c8d-9396-557b2c05a30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 10:06:45.638 | INFO     | __main__:<module>:15 -        sepal_length  sepal_width  petal_length  petal_width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.057333      3.758000     1.199333\n",
      "std        0.828066     0.435866      1.765298     0.762238\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n",
      "2021-06-17 10:06:45.657 | INFO     | __main__:<module>:39 - {'setosa': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 10}, 'versicolor': {'precision': 1.0, 'recall': 0.8888888888888888, 'f1-score': 0.9411764705882353, 'support': 9}, 'virginica': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1-score': 0.9565217391304348, 'support': 11}, 'accuracy': 0.9666666666666667, 'macro avg': {'precision': 0.9722222222222222, 'recall': 0.9629629629629629, 'f1-score': 0.9658994032395567, 'support': 30}, 'weighted avg': {'precision': 0.9694444444444444, 'recall': 0.9666666666666667, 'f1-score': 0.9664109121909632, 'support': 30}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from loguru import logger\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report as clf_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "output_dir = \"first_try\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extract data\n",
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "raw_data_statistics = raw_data.describe()\n",
    "logger.info(raw_data_statistics)\n",
    "\n",
    "# prepare\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "standardize = False\n",
    "X = raw_data[features]\n",
    "y = raw_data[\"species\"]\n",
    "\n",
    "mean = X.mean()\n",
    "std = X.std()\n",
    "\n",
    "if standardize:\n",
    "    logger.info(\"Standardize X.\")\n",
    "    X = (X - mean) / std\n",
    "\n",
    "# train\n",
    "clf_params = {\"max_depth\": 2}\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.8)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42, **clf_params)\n",
    "clf.fit(X_train, y_train)\n",
    "joblib.dump(clf, f\"{output_dir}/model.joblib\")\n",
    "y_pred = clf.predict(X_test)\n",
    "classification_report = clf_report(y_true=y_test, y_pred=y_pred, output_dict=True)\n",
    "logger.info(classification_report)\n",
    "pd.DataFrame(classification_report).to_csv(f\"{output_dir}/classification_report.csv\")\n",
    "\n",
    "# validate\n",
    "macro_avg_f1_score_min = 0.95\n",
    "macro_avg_f1_score = classification_report[\"macro avg\"][\"f1-score\"] \n",
    "\n",
    "if macro_avg_f1_score < macro_avg_f1_score_min:\n",
    "    passed = False\n",
    "else: \n",
    "    passed = True\n",
    "print(passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a1ac4-d74c-4eea-8ff5-476b0519de5b",
   "metadata": {},
   "source": [
    "### Resource considerations for the pipeline\n",
    "\n",
    "- `extract`: Might be Gigabytes of data to load. **time: max low double digit minutes**\n",
    "- `prepare`: Can become quite complex, especially if we do feature engineering. **time: up to double digit hours**\n",
    "- `train`: Can become quite complex. **time: up to double digit hours**\n",
    "- `validate`: Should be kept simple. **time: at most single digit minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da191510-9f05-497b-ae19-0f1df3e80da3",
   "metadata": {},
   "source": [
    "### Automation Option 1: Lift the \"monolithic\" pipeline \n",
    "- Put the code above as-is into a docker container\n",
    "- Run it on a machine that has as many resources as the most demanding step requires\n",
    "- Any error during execution will require to start the whole execution again\n",
    "\n",
    "Pro: \n",
    "- Can be done fast\n",
    "\n",
    "Con:\n",
    "- not robust at all\n",
    "- if we want to test it, we need to run the whole pipeline \n",
    "- Temptation for \"let me add a small improvement here\" is very big\n",
    "- No intermediate checks, mistakes can be easily overseen\n",
    "- Possibly expensive, because we need maximum resources during full pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0eb6fa-4026-4d39-89bd-4d69450f718b",
   "metadata": {},
   "source": [
    "### Automation Option 2: Refactor to a robust pipeline by creating independent, well-defined components\n",
    "\n",
    "- A pipeline is composed of independent components which are [pure functions](https://en.wikipedia.org/wiki/Pure_function)\n",
    "- Independent components are thoroughly (unit-)tested (todo in this notebook)\n",
    "- The pipeline should have a functional end to end test (todo in this notebook)\n",
    "- All artifacts and metadata are persisted in a unique storage location (for example at pipeline run UUID)\n",
    "- Config files should be used\n",
    "\n",
    "Pro:\n",
    "- robust\n",
    "- transparent, easy to track\n",
    "- easy to port to a ML framework\n",
    "- Scales with data\n",
    "- Scales with team size\n",
    "- Makes the team in the mid run more productive\n",
    "- Possibly more cost-effective during execution, because we can assign resources more fine-granular\n",
    "\n",
    "Con:\n",
    "- Requires (software) engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a284c9c3-0d04-4cd5-8072-182fd703a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_id = \"first_pipeline_run\"\n",
    "output_dir = pipeline_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a3366-12f7-4f90-9130-862c25d36168",
   "metadata": {},
   "source": [
    "### We create a directory for the pipeline run outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075ec021-ab92-41c2-ae7f-441e9d2630c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c450b5-ff79-4792-8993-4cade0751dfa",
   "metadata": {},
   "source": [
    "### First develop interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5bf3b01-1ecf-49a7-b56c-017b6a99bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "raw_data_path = f\"{output_dir}/raw_data.parquet\"\n",
    "raw_data.to_parquet(raw_data_path)\n",
    "\n",
    "raw_data_statistics = raw_data.describe()\n",
    "raw_data_statistics_path = f\"{output_dir}/raw_data_statistics.csv\"\n",
    "raw_data_statistics.to_csv(raw_data_statistics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0dc719e-e871-4a9f-b0c4-5ff2ab876d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it works, delete the contents\n",
    "!rm -r $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0df627-48a3-4115-bc10-25969dde1a0d",
   "metadata": {},
   "source": [
    "### Then turn it into a pure, self-contained function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36405d75-8914-4330-af07-1b3882ff731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(output_dir: str) -> dict:\n",
    "    params = locals()\n",
    "    from loguru import logger\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import uuid\n",
    "    # BEST PRACTICE: Make \"started\" and \"finished\" logs\n",
    "    # BEST PRACTICE: Log the input params of a function\n",
    "    logger.info(f\"extract started with {params}.\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    raw_data = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "    raw_data_path = f\"{output_dir}/raw_data.parquet\"\n",
    "    raw_data.to_parquet(raw_data_path)\n",
    "    \n",
    "    raw_data_statistics = raw_data.describe()\n",
    "    raw_data_statistics_path = f\"{output_dir}/raw_data_statistics.csv\"\n",
    "    raw_data_statistics.to_csv(raw_data_statistics_path)\n",
    "    logger.info(\"extract finished.\")\n",
    "    return {\n",
    "        \"raw_data_path\": raw_data_path,\n",
    "        \"raw_data_statistics_path\": raw_data_statistics_path,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d30597-0fa1-4a89-9d5b-7750d983634a",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482f9dbc-27ba-484b-9a25-368eaeb98380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 10:06:46.056 | INFO     | __main__:extract:9 - extract started with {'output_dir': 'first_pipeline_run'}.\n",
      "2021-06-17 10:06:46.234 | INFO     | __main__:extract:19 - extract finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'raw_data_path': 'first_pipeline_run/raw_data.parquet',\n",
       " 'raw_data_statistics_path': 'first_pipeline_run/raw_data_statistics.csv'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_result = extract(output_dir=output_dir)\n",
    "extract_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23527d7c-1ac4-499f-9f25-1608f88b9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(raw_data_path: str, features: list, standardize: bool, output_dir: str) -> dict:\n",
    "    \"\"\"Prepare the selected features and standardize if wanted.\"\"\"\n",
    "    params = locals()\n",
    "    from loguru import logger\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    logger.info(f\"prepare started with {params}\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    raw_data = pd.read_parquet(raw_data_path)\n",
    "    X = raw_data[features]\n",
    "    y = raw_data[\"species\"]\n",
    "\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "\n",
    "    if standardize:\n",
    "        logger.info(\"Standardize X.\")\n",
    "        X = (X - mean) / std\n",
    "\n",
    "    X_path = f\"{output_dir}/X.parquet\"\n",
    "    y_path = f\"{output_dir}/y.parquet\"\n",
    "\n",
    "    X.to_parquet(X_path)\n",
    "    y.to_frame().to_parquet(y_path)\n",
    "    logger.info(\"prepare finished.\")\n",
    "    return {\n",
    "        \"mean\": mean.to_dict(),\n",
    "        \"std\": std.to_dict(),\n",
    "        \"X_path\": X_path,\n",
    "        \"y_path\": y_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0799044-3ad5-48f2-96c7-3debcab8b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 10:06:46.256 | INFO     | __main__:prepare:7 - prepare started with {'raw_data_path': 'first_pipeline_run/raw_data.parquet', 'features': ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], 'standardize': True, 'output_dir': 'first_pipeline_run'}\n",
      "2021-06-17 10:06:46.270 | INFO     | __main__:prepare:19 - Standardize X.\n",
      "2021-06-17 10:06:46.277 | INFO     | __main__:prepare:27 - prepare finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': {'sepal_length': 5.843333333333334,\n",
       "  'sepal_width': 3.0573333333333337,\n",
       "  'petal_length': 3.7580000000000005,\n",
       "  'petal_width': 1.1993333333333336},\n",
       " 'std': {'sepal_length': 0.828066127977863,\n",
       "  'sepal_width': 0.4358662849366982,\n",
       "  'petal_length': 1.7652982332594662,\n",
       "  'petal_width': 0.7622376689603465},\n",
       " 'X_path': 'first_pipeline_run/X.parquet',\n",
       " 'y_path': 'first_pipeline_run/y.parquet'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_result = prepare(raw_data_path=extract_result[\"raw_data_path\"], features=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], standardize=True, output_dir=pipeline_run_id)\n",
    "prepare_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd9706d-c9c6-46be-96ef-edb88fa6f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_path: str, y_path: str, output_dir: str, clf_params: dict) -> dict:\n",
    "    params = locals()\n",
    "    from joblib import dump\n",
    "    from loguru import logger\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import classification_report as clf_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    logger.info(f\"train started with {params}.\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    X = pd.read_parquet(X_path)\n",
    "    y = pd.read_parquet(y_path)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.8)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(random_state=42, **clf_params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    classification_report = clf_report(y_true=y_test, y_pred=y_pred, output_dict=True)\n",
    "    model_path = f\"{output_dir}/model.joblib\"\n",
    "    dump(clf, model_path)\n",
    "    logger.info(\"train finished.\")\n",
    "    return {\n",
    "        \"model_path\": model_path,\n",
    "        \"classification_report\": classification_report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b0656f-a01c-4ff4-a01f-a68ce900dbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 10:06:46.293 | INFO     | __main__:train:10 - train started with {'X_path': 'first_pipeline_run/X.parquet', 'y_path': 'first_pipeline_run/y.parquet', 'output_dir': 'first_pipeline_run', 'clf_params': {'max_depth': 2}}.\n",
      "2021-06-17 10:06:46.315 | INFO     | __main__:train:24 - train finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_path': 'first_pipeline_run/model.joblib',\n",
       " 'classification_report': {'setosa': {'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1-score': 1.0,\n",
       "   'support': 10},\n",
       "  'versicolor': {'precision': 1.0,\n",
       "   'recall': 0.8888888888888888,\n",
       "   'f1-score': 0.9411764705882353,\n",
       "   'support': 9},\n",
       "  'virginica': {'precision': 0.9166666666666666,\n",
       "   'recall': 1.0,\n",
       "   'f1-score': 0.9565217391304348,\n",
       "   'support': 11},\n",
       "  'accuracy': 0.9666666666666667,\n",
       "  'macro avg': {'precision': 0.9722222222222222,\n",
       "   'recall': 0.9629629629629629,\n",
       "   'f1-score': 0.9658994032395567,\n",
       "   'support': 30},\n",
       "  'weighted avg': {'precision': 0.9694444444444444,\n",
       "   'recall': 0.9666666666666667,\n",
       "   'f1-score': 0.9664109121909632,\n",
       "   'support': 30}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = train(X_path=prepare_result[\"X_path\"], y_path=prepare_result[\"y_path\"], output_dir=pipeline_run_id, clf_params={\"max_depth\": 2})\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cda8e1b-a94c-49dc-bc3a-d5e8a43f2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(classification_report: dict, macro_avg_f1_score_min: float) -> dict:\n",
    "    params = locals()\n",
    "    from loguru import logger\n",
    "    logger.info(\"validate started.\")\n",
    "    macro_avg_f1_score = classification_report[\"macro avg\"][\"f1-score\"] \n",
    "    \n",
    "    if macro_avg_f1_score < macro_avg_f1_score_min:\n",
    "        passed = False\n",
    "    else: \n",
    "        passed = True\n",
    "    logger.info(\"validate finished.\")\n",
    "    return {\n",
    "        \"passed\": passed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c108d0b4-1301-4041-a231-0c567868fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 10:06:46.330 | INFO     | __main__:validate:4 - validate started.\n",
      "2021-06-17 10:06:46.331 | INFO     | __main__:validate:11 - validate finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'passed': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_result = validate(classification_report=train_result[\"classification_report\"], macro_avg_f1_score_min=0.95)\n",
    "validate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2985d5-9a01-4332-a573-32085849a6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 10:06:46.336 | INFO     | __main__:validate:4 - validate started.\n",
      "2021-06-17 10:06:46.337 | INFO     | __main__:validate:11 - validate finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'passed': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_result = validate(classification_report=train_result[\"classification_report\"], macro_avg_f1_score_min=0.99)\n",
    "validate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c277652-2cb8-4fcc-a514-df850967be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_file(dirname: str, d: dict):\n",
    "    import os\n",
    "    import json\n",
    "    from loguru import logger\n",
    "    \n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    filename = f\"{dirname}/output.json\"\n",
    "    logger.info(f\"Write {d} to {filename}.\")\n",
    "    with open(filename, \"w\", encoding=\"UTF-8\") as f:\n",
    "        f.write(json.dumps(d))\n",
    "    logger.info(\"Write finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d3284-6438-40d2-b51d-6053efbbb9b1",
   "metadata": {},
   "source": [
    "### Define a pipeline that puts the steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9335a08-21b1-4f81-9a0c-5eaae5de373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from loguru import logger\n",
    "def pipeline(\n",
    "    output_dir: str,\n",
    "    prepare_features: list,\n",
    "    prepare_standardize: bool,\n",
    "    train_clf_params: dict,\n",
    "    validate_macro_avg_f1_score_min: float\n",
    "):\n",
    "    params = locals()\n",
    "    if not output_dir:\n",
    "        output_dir = f\"output/{str(uuid.uuid4())}\"\n",
    "    dict_to_file(output_dir, params)\n",
    "\n",
    "    extract_dir = f\"{output_dir}/extract\"\n",
    "    extract_result = extract(output_dir=extract_dir)\n",
    "    dict_to_file(extract_dir, extract_result)\n",
    "    \n",
    "    prepare_dir = f\"{output_dir}/prepare\"\n",
    "    prepare_result = prepare(raw_data_path=extract_result[\"raw_data_path\"], features=prepare_features, standardize=prepare_standardize, output_dir=prepare_dir)\n",
    "    dict_to_file(prepare_dir, prepare_result)\n",
    "\n",
    "    train_dir = f\"{output_dir}/train\"\n",
    "    train_result = train(X_path=prepare_result[\"X_path\"], y_path=prepare_result[\"y_path\"], output_dir=train_dir, clf_params=train_clf_params)\n",
    "    dict_to_file(train_dir, train_result)\n",
    "\n",
    "    validate_dir = f\"{output_dir}/validate\"\n",
    "    validate_result = validate(classification_report=train_result[\"classification_report\"], macro_avg_f1_score_min=validate_macro_avg_f1_score_min)\n",
    "    dict_to_file(validate_dir, validate_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b83c6087-b2bd-43cc-9516-75da15c1d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd51a44a-5a6f-4619-967a-4c406904bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': None,\n",
       " 'prepare': {'features': ['sepal_length',\n",
       "   'sepal_width',\n",
       "   'petal_length',\n",
       "   'petal_width'],\n",
       "  'standardize': False},\n",
       " 'train': {'clf_params': {'criterion': 'gini', 'max_depth': 2}},\n",
       " 'validate': {'macro_avg_f1_score_min': 0.95}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config/run1.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "735f73f7-f88a-4c19-ad9e-19bd0395215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 10:06:46.390 | INFO     | __main__:dict_to_file:8 - Write {'output_dir': None, 'prepare_features': ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], 'prepare_standardize': False, 'train_clf_params': {'criterion': 'gini', 'max_depth': 2}, 'validate_macro_avg_f1_score_min': 0.95} to output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/output.json.\n",
      "2021-06-17 10:06:46.392 | INFO     | __main__:dict_to_file:11 - Write finished.\n",
      "2021-06-17 10:06:46.393 | INFO     | __main__:extract:9 - extract started with {'output_dir': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/extract'}.\n",
      "2021-06-17 10:06:46.525 | INFO     | __main__:extract:19 - extract finished.\n",
      "2021-06-17 10:06:46.526 | INFO     | __main__:dict_to_file:8 - Write {'raw_data_path': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/extract/raw_data.parquet', 'raw_data_statistics_path': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/extract/raw_data_statistics.csv'} to output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/extract/output.json.\n",
      "2021-06-17 10:06:46.527 | INFO     | __main__:dict_to_file:11 - Write finished.\n",
      "2021-06-17 10:06:46.528 | INFO     | __main__:prepare:7 - prepare started with {'raw_data_path': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/extract/raw_data.parquet', 'features': ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], 'standardize': False, 'output_dir': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/prepare'}\n",
      "2021-06-17 10:06:46.536 | INFO     | __main__:prepare:27 - prepare finished.\n",
      "2021-06-17 10:06:46.537 | INFO     | __main__:dict_to_file:8 - Write {'mean': {'sepal_length': 5.843333333333334, 'sepal_width': 3.0573333333333337, 'petal_length': 3.7580000000000005, 'petal_width': 1.1993333333333336}, 'std': {'sepal_length': 0.828066127977863, 'sepal_width': 0.4358662849366982, 'petal_length': 1.7652982332594662, 'petal_width': 0.7622376689603465}, 'X_path': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/prepare/X.parquet', 'y_path': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/prepare/y.parquet'} to output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/prepare/output.json.\n",
      "2021-06-17 10:06:46.537 | INFO     | __main__:dict_to_file:11 - Write finished.\n",
      "2021-06-17 10:06:46.538 | INFO     | __main__:train:10 - train started with {'X_path': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/prepare/X.parquet', 'y_path': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/prepare/y.parquet', 'output_dir': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/train', 'clf_params': {'criterion': 'gini', 'max_depth': 2}}.\n",
      "2021-06-17 10:06:46.551 | INFO     | __main__:train:24 - train finished.\n",
      "2021-06-17 10:06:46.552 | INFO     | __main__:dict_to_file:8 - Write {'model_path': 'output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/train/model.joblib', 'classification_report': {'setosa': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 10}, 'versicolor': {'precision': 1.0, 'recall': 0.8888888888888888, 'f1-score': 0.9411764705882353, 'support': 9}, 'virginica': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1-score': 0.9565217391304348, 'support': 11}, 'accuracy': 0.9666666666666667, 'macro avg': {'precision': 0.9722222222222222, 'recall': 0.9629629629629629, 'f1-score': 0.9658994032395567, 'support': 30}, 'weighted avg': {'precision': 0.9694444444444444, 'recall': 0.9666666666666667, 'f1-score': 0.9664109121909632, 'support': 30}}} to output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/train/output.json.\n",
      "2021-06-17 10:06:46.553 | INFO     | __main__:dict_to_file:11 - Write finished.\n",
      "2021-06-17 10:06:46.553 | INFO     | __main__:validate:4 - validate started.\n",
      "2021-06-17 10:06:46.553 | INFO     | __main__:validate:11 - validate finished.\n",
      "2021-06-17 10:06:46.554 | INFO     | __main__:dict_to_file:8 - Write {'passed': True} to output/9d2f962c-d02a-433b-ad2a-e396a2fcab5d/validate/output.json.\n",
      "2021-06-17 10:06:46.555 | INFO     | __main__:dict_to_file:11 - Write finished.\n"
     ]
    }
   ],
   "source": [
    "pipeline(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    prepare_features=config[\"prepare\"][\"features\"],\n",
    "    prepare_standardize=config[\"prepare\"][\"standardize\"],\n",
    "    train_clf_params=config[\"train\"][\"clf_params\"],\n",
    "    validate_macro_avg_f1_score_min=config[\"validate\"][\"macro_avg_f1_score_min\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7c52a14-25c8-4f0b-bd38-dfc73d79a714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': None,\n",
       " 'prepare': {'features': ['sepal_length', 'sepal_width'], 'standardize': True},\n",
       " 'train': {'clf_params': {'criterion': 'gini', 'max_depth': 10}},\n",
       " 'validate': {'macro_avg_f1_score_min': 0.95}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config/run2.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7adb79f2-9387-4369-9388-cd1cb0d6d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 10:06:46.567 | INFO     | __main__:dict_to_file:8 - Write {'output_dir': None, 'prepare_features': ['sepal_length', 'sepal_width'], 'prepare_standardize': True, 'train_clf_params': {'criterion': 'gini', 'max_depth': 10}, 'validate_macro_avg_f1_score_min': 0.95} to output/5c8aea30-9055-4852-83e3-6e6029d71093/output.json.\n",
      "2021-06-17 10:06:46.568 | INFO     | __main__:dict_to_file:11 - Write finished.\n",
      "2021-06-17 10:06:46.568 | INFO     | __main__:extract:9 - extract started with {'output_dir': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/extract'}.\n",
      "2021-06-17 10:06:46.758 | INFO     | __main__:extract:19 - extract finished.\n",
      "2021-06-17 10:06:46.759 | INFO     | __main__:dict_to_file:8 - Write {'raw_data_path': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/extract/raw_data.parquet', 'raw_data_statistics_path': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/extract/raw_data_statistics.csv'} to output/5c8aea30-9055-4852-83e3-6e6029d71093/extract/output.json.\n",
      "2021-06-17 10:06:46.760 | INFO     | __main__:dict_to_file:11 - Write finished.\n",
      "2021-06-17 10:06:46.761 | INFO     | __main__:prepare:7 - prepare started with {'raw_data_path': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/extract/raw_data.parquet', 'features': ['sepal_length', 'sepal_width'], 'standardize': True, 'output_dir': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/prepare'}\n",
      "2021-06-17 10:06:46.771 | INFO     | __main__:prepare:19 - Standardize X.\n",
      "2021-06-17 10:06:46.776 | INFO     | __main__:prepare:27 - prepare finished.\n",
      "2021-06-17 10:06:46.777 | INFO     | __main__:dict_to_file:8 - Write {'mean': {'sepal_length': 5.843333333333334, 'sepal_width': 3.0573333333333337}, 'std': {'sepal_length': 0.828066127977863, 'sepal_width': 0.4358662849366982}, 'X_path': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/prepare/X.parquet', 'y_path': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/prepare/y.parquet'} to output/5c8aea30-9055-4852-83e3-6e6029d71093/prepare/output.json.\n",
      "2021-06-17 10:06:46.778 | INFO     | __main__:dict_to_file:11 - Write finished.\n",
      "2021-06-17 10:06:46.779 | INFO     | __main__:train:10 - train started with {'X_path': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/prepare/X.parquet', 'y_path': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/prepare/y.parquet', 'output_dir': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/train', 'clf_params': {'criterion': 'gini', 'max_depth': 10}}.\n",
      "2021-06-17 10:06:46.799 | INFO     | __main__:train:24 - train finished.\n",
      "2021-06-17 10:06:46.801 | INFO     | __main__:dict_to_file:8 - Write {'model_path': 'output/5c8aea30-9055-4852-83e3-6e6029d71093/train/model.joblib', 'classification_report': {'setosa': {'precision': 1.0, 'recall': 0.9, 'f1-score': 0.9473684210526316, 'support': 10}, 'versicolor': {'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333, 'support': 9}, 'virginica': {'precision': 0.5, 'recall': 0.5454545454545454, 'f1-score': 0.5217391304347826, 'support': 11}, 'accuracy': 0.6, 'macro avg': {'precision': 0.611111111111111, 'recall': 0.592929292929293, 'f1-score': 0.6008136282735825, 'support': 30}, 'weighted avg': {'precision': 0.6166666666666667, 'recall': 0.6, 'f1-score': 0.6070938215102974, 'support': 30}}} to output/5c8aea30-9055-4852-83e3-6e6029d71093/train/output.json.\n",
      "2021-06-17 10:06:46.802 | INFO     | __main__:dict_to_file:11 - Write finished.\n",
      "2021-06-17 10:06:46.802 | INFO     | __main__:validate:4 - validate started.\n",
      "2021-06-17 10:06:46.803 | INFO     | __main__:validate:11 - validate finished.\n",
      "2021-06-17 10:06:46.803 | INFO     | __main__:dict_to_file:8 - Write {'passed': False} to output/5c8aea30-9055-4852-83e3-6e6029d71093/validate/output.json.\n",
      "2021-06-17 10:06:46.804 | INFO     | __main__:dict_to_file:11 - Write finished.\n"
     ]
    }
   ],
   "source": [
    "pipeline(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    prepare_features=config[\"prepare\"][\"features\"],\n",
    "    prepare_standardize=config[\"prepare\"][\"standardize\"],\n",
    "    train_clf_params=config[\"train\"][\"clf_params\"],\n",
    "    validate_macro_avg_f1_score_min=config[\"validate\"][\"macro_avg_f1_score_min\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d7706-8f1b-4a2c-b70e-87de15d22008",
   "metadata": {},
   "source": [
    "## Automate and go on!\n",
    "\n",
    "- The goal is **unattended exeuction**. This frees up our resources to focus on the next projects\n",
    "- Implement basic monitoring that **notifies us on pipeline failure only**. Notifications for successful pipeline runs are only noise. \n",
    "- **Gain confidence in the pipeline** by writing end-to-end tests on artificial data with output verification (is the output as expected?)\n",
    "- **Schedule** periodically for retraining on the freshest data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0648269-a9fa-426f-bad4-d46928bd81ef",
   "metadata": {},
   "source": [
    "### Principles of a robust pipeline:\n",
    "- All artifacts and metadata are persisted in a unique storage location (for example at pipeline run UUID)\n",
    "- Config files are crucial\n",
    "- A pipeline is composed of independent components\n",
    "- Independent components are thoroughly (unit-)tested (todo in this notebook)\n",
    "- The pipeline should have a functional end to end test (todo in this notebook\n",
    "\n",
    "### Move away from own machine\n",
    "\n",
    "#### Option 1: Run \"locally\" on a remote server\n",
    "- Start a remote server, download the git repo there, and execute there\n",
    "- For scheduling, custom CRON jobs need to be created\n",
    "- File storage location: file system of the remote machine\n",
    "\n",
    "#### Option 2: Use ML Frameworks\n",
    "- Package the Python code in a docker container\n",
    "- Write a wrapper around each component using the SDK of the orchestrator of your choice\n",
    "- You get a nice UI to browse previous and planned executions\n",
    "- Requires quite some engineering\n",
    "- Persistence of Artifacts and metadata will then be done in a database or on remove storage\n",
    "- List of orchestrators (not exhaustive):\n",
    "    - [Luigi](https://github.com/spotify/luigi), very mature, maybe outdated, can run components on a single machine possible\n",
    "    - [MLFlow](https://www.mlflow.org/docs/latest/projects.html#running-projects), actively developed, nice UI, can start a pipeline on a single machine or different machines\n",
    "    - [Airflow](https://airflow.apache.org/docs/apache-airflow/2.0.1/), very powerful tool, (too?) verbose UI, overkill for simple tasks, can be run on kubernetes\n",
    "    - [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/pipelines-overview/), powerful, reduced UI, kubernetes only, overkill for simple tasks\n",
    "- Managed orchestrator offerings exist from Azure, GCP, and also other service providers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab7df6-53e4-41aa-b4fd-b5e79e34e807",
   "metadata": {},
   "source": [
    "## Tips and Tricks\n",
    "\n",
    "- Keep the overall goal in mind: Automate and go on with the next project\n",
    "- We write code for humans, not for machines\n",
    "- We write the code for our coworkers\n",
    "- Each component should be independent\n",
    "- Persist all outputs and metadata!\n",
    "- After each pipeline step (= component execution), return where to find the outputs\n",
    "- Develop a template for ML pipeline components\n",
    "- Verbosity is your friend\n",
    "- Use typed Python\n",
    "- Do NOT use CSV as serialization format, use Parquet instead. CSV does not scale to bigger data.\n",
    "- lego variable naming improves code maintenance\n",
    "- be consistent with variable and file naming, do not introduce unnecessary variable renames (like `m_d = params[\"max_depth\"]`)\n",
    "- consistency is very important, things should always be done one way (for example: file path locations)\n",
    "- Write unit tests for every component\n",
    "- Write an end2end test with artificial data and verify the output\n",
    "- logging (NOT printing) is important\n",
    "- Only one change at a time! Isolate the changes to investigate the outut for the pipeline\n",
    "- Create a new unique directory for the outputs of every pipeline run, no data will ever be overwritten\n",
    "- What happens in the cloud? -> each component gets its own container. Files /artifacts are written to cloud storage, and metadata is written to a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7ce9ed-1b30-428d-8753-77230d27a9ef",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d2d0b-18bd-456f-b536-f747b14b41da",
   "metadata": {},
   "source": [
    "- https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning, MLOps for advanced automation for big teams with big data and multiple models per week\n",
    "- https://mlflow.org/, Framework with nice UI for model training and experimentation, can be executed locally\n",
    "- https://dvc.org/, Data version control, “Git” for data\n",
    "- https://dvc.org/doc/use-cases/versioning-data-and-model-files/tutorial, Tutorial for using “dvc” like the “git” command\n",
    "- https://python-poetry.org/, in our experience, best Python virtual environment and packaging tool\n",
    "- https://www.tensorflow.org/tfx, Opinionated, complex, tensorflow-based pipeline framework\n",
    "- https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines, Azure ML pipelines with GUI for designer\n",
    "- https://medium.com/memory-leak/data-orchestration-a-primer-56f3ddbb1700, Data Orchestration Landscape\n",
    "- https://github.com/Azure/MachineLearningNotebooks/tree/master/tutorials, Example notebooks for Azure ML\n",
    "- https://cloud.google.com/composer, Google Cloud managed Airflow\n",
    "- https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline, Googles brand-new (in preview!) Pipelines tooling, pay-per-use for cloud resources, easy schedule pipelines for retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8564518-250f-401b-88d4-a5d7d268332c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
