{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa4ca38-cc2a-4d7b-95cf-d14abcd13d33",
   "metadata": {},
   "source": [
    "# Simple and Robust ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed84ba3-570e-4b65-b12d-3c9550ae0e96",
   "metadata": {},
   "source": [
    "With this notebook, I want to highlight the fundamentals of a robust ML pipeline.\n",
    "They are:\n",
    "- Steps are [pure functions](https://en.wikipedia.org/wiki/Pure_function)\n",
    "- Keep the functionality in a step small\n",
    "- Persistence between the steps\n",
    "- Onion-like abstraction layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3450010-6e6f-4f53-9662-ecd8a2795669",
   "metadata": {},
   "source": [
    "ML Code is just a small part when doing Machine Learning\n",
    "\n",
    "Original Source: *Sculley, David, et al. \"Hidden technical debt in machine learning systems.\" Advances in neural information processing systems 28 (2015): 2503-2511.*\n",
    "\n",
    "![ML Code is just a small part when doing Machine Learning](https://cloud.google.com/architecture/images/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning-1-elements-of-ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57c099-f15a-4374-870a-e561fa7e9d6f",
   "metadata": {},
   "source": [
    "![How a data pipeline looks like](https://cloud.google.com/architecture/images/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning-2-manual-ml.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2fd5c-55b0-4cc1-a02a-f98c5b9593c5",
   "metadata": {},
   "source": [
    "# This is about workflow and tooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806564e9-5dd7-492e-9d88-fa5af35fd8f5",
   "metadata": {},
   "source": [
    "- We write this code for humans, not for machines\n",
    "- We write the code for our coworkers\n",
    "- Each component should be independent\n",
    "- Persist all outputs and metadata!\n",
    "- Verbosity is your friend\n",
    "- Use typed Python\n",
    "- Do use CSV as serialization between steps only for small data. Parquet is the better option for bigger data\n",
    "- lego variable naming improves traceability\n",
    "- be consistent with variable and file naming\n",
    "- After each step, return where to find the outputs\n",
    "- consistency is very important, things should always be only one way\n",
    "- It is about bringing habits to code\n",
    "- Write unit tests for every component\n",
    "- Write an end2end test with artificial data and verify the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243a9a1-b5ee-4cdc-ae84-75273f0f54f0",
   "metadata": {},
   "source": [
    "This is your small pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eedb366-7bd8-4fb0-afa4-e0914485db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2573a8eb-c7fe-489a-af72-e8d115d4b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_hash():\n",
    "    return uuid.uuid4().split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253c3271-f2e0-44f5-87ce-8f54220d8b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a284c9c3-0d04-4cd5-8072-182fd703a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_id = \"first_pipeline_run\"\n",
    "# In the future: pipeline_run_id = str(uuid.uuid4())\n",
    "output_dir = pipeline_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a3366-12f7-4f90-9130-862c25d36168",
   "metadata": {},
   "source": [
    "### We create a directory for the pipeline run outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075ec021-ab92-41c2-ae7f-441e9d2630c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c450b5-ff79-4792-8993-4cade0751dfa",
   "metadata": {},
   "source": [
    "### [Methods] First develop interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bf3b01-1ecf-49a7-b56c-017b6a99bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "extract_output_dir = f\"{output_dir}/extract\"\n",
    "os.makedirs(extract_output_dir)\n",
    "\n",
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "raw_data_path = f\"{extract_output_dir}/raw_data.parquet\"\n",
    "raw_data.to_parquet(raw_data_path)\n",
    "\n",
    "raw_data_statistics = raw_data.describe()\n",
    "raw_data_statistics_path = f\"{extract_output_dir}/raw_data_statistics.csv\"\n",
    "raw_data_statistics.to_csv(raw_data_statistics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dc719e-e871-4a9f-b0c4-5ff2ab876d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the contents\n",
    "!rm -r $extract_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0df627-48a3-4115-bc10-25969dde1a0d",
   "metadata": {},
   "source": [
    "### [Methods] Then turn it into a self-contained function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36405d75-8914-4330-af07-1b3882ff731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(output_dir: str) -> dict:\n",
    "    params = locals()\n",
    "    from loguru import logger\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import uuid\n",
    "    logger.info(f\"extract started with {params}.\")\n",
    "\n",
    "    extract_output_dir = f\"{output_dir}/extract\"\n",
    "    os.makedirs(extract_output_dir)\n",
    "    \n",
    "    raw_data = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "    raw_data_path = f\"{extract_output_dir}/raw_data.parquet\"\n",
    "    raw_data.to_parquet(raw_data_path)\n",
    "    \n",
    "    raw_data_statistics = raw_data.describe()\n",
    "    raw_data_statistics_path = f\"{extract_output_dir}/raw_data_statistics.csv\"\n",
    "    raw_data_statistics.to_csv(raw_data_statistics_path)\n",
    "    logger.info(\"extract finished.\")\n",
    "    return {\n",
    "        \"raw_data_path\": raw_data_path,\n",
    "        \"raw_data_statistics_path\": raw_data_statistics_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d30597-0fa1-4a89-9d5b-7750d983634a",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "482f9dbc-27ba-484b-9a25-368eaeb98380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 00:03:54.393 | INFO     | __main__:extract:5 - Load data started.\n",
      "2021-06-17 00:03:54.522 | INFO     | __main__:extract:16 - Load data finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'raw_data_path': 'first_pipeline_run/extract/raw_data.parquet',\n",
       " 'raw_data_statistics_path': 'first_pipeline_run/extract/raw_data_statistics.csv'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_result = extract(output_dir=output_dir)\n",
    "extract_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23527d7c-1ac4-499f-9f25-1608f88b9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(raw_data_path: str, features: list, standardize: bool, output_dir: str) -> dict:\n",
    "    \"\"\"Prepare the selected features and standardize if wanted.\"\"\"\n",
    "    params = locals()\n",
    "    from loguru import logger\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    logger.info(f\"prepare started with {params}\")\n",
    "\n",
    "    prepare_output_dir = f\"{output_dir}/prepare\"\n",
    "    os.makedirs(prepare_output_dir)\n",
    "\n",
    "    raw_data = pd.read_parquet(raw_data_path)\n",
    "    X = raw_data[features]\n",
    "    y = raw_data[\"species\"]\n",
    "\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "\n",
    "    if standardize:\n",
    "        logger.info(\"Standardize X.\")\n",
    "        X = (X - mean) / std\n",
    "\n",
    "    X_path = f\"{prepare_output_dir}/X.parquet\"\n",
    "    y_path = f\"{prepare_output_dir}/y.parquet\"\n",
    "\n",
    "    X.to_parquet(X_path)\n",
    "    y.to_frame().to_parquet(y_path)\n",
    "    logger.info(\"prepare finished.\")\n",
    "    return {\n",
    "        \"mean\": mean.to_dict(),\n",
    "        \"std\": std.to_dict(),\n",
    "        \"X_path\": X_path,\n",
    "        \"y_path\": y_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0799044-3ad5-48f2-96c7-3debcab8b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 00:10:12.673 | INFO     | __main__:prepare:7 - prepare started with {'raw_data_path': 'first_pipeline_run/extract/raw_data.parquet', 'features': ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], 'standardize': True, 'output_dir': 'first_pipeline_run'}\n",
      "2021-06-17 00:10:12.692 | INFO     | __main__:prepare:26 - prepare finished.\n"
     ]
    }
   ],
   "source": [
    "prepare_result = prepare(raw_data_path=extract_result[\"raw_data_path\"], features=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], standardize=True, output_dir=pipeline_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfd9706d-c9c6-46be-96ef-edb88fa6f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_path: str, y_path: str, output_dir: str, clf_params: dict) -> dict:\n",
    "    params = locals()\n",
    "    from joblib import dump\n",
    "    from loguru import logger\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import classification_report as clf_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    logger.info(f\"train started with {params}.\")\n",
    "    train_output_dir = f\"{output_dir}/train\"\n",
    "    os.makedirs(train_output_dir)\n",
    "\n",
    "    X = pd.read_parquet(X_path)\n",
    "    y = pd.read_parquet(y_path)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.8)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(random_state=42, **clf_params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    classification_report = clf_report(y_true=y_test, y_pred=y_pred, output_dict=True)\n",
    "    model_path = f\"{train_output_dir}/model.joblib\"\n",
    "    dump(clf, model_path)\n",
    "    logger.info(\"train finished.\")\n",
    "    return {\n",
    "        \"model_path\": model_path,\n",
    "        \"classification_report\": classification_report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79b0656f-a01c-4ff4-a01f-a68ce900dbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 00:15:08.638 | INFO     | __main__:train:10 - train started with {'X_path': 'first_pipeline_run/prepare/X.parquet', 'y_path': 'first_pipeline_run/prepare/y.parquet', 'output_dir': 'dudu', 'clf_params': {'max_depth': 2}}.\n",
      "2021-06-17 00:15:08.659 | INFO     | __main__:train:25 - train finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_path': 'dudu/train/model.joblib',\n",
       " 'classification_report': {'setosa': {'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1-score': 1.0,\n",
       "   'support': 10},\n",
       "  'versicolor': {'precision': 1.0,\n",
       "   'recall': 0.8888888888888888,\n",
       "   'f1-score': 0.9411764705882353,\n",
       "   'support': 9},\n",
       "  'virginica': {'precision': 0.9166666666666666,\n",
       "   'recall': 1.0,\n",
       "   'f1-score': 0.9565217391304348,\n",
       "   'support': 11},\n",
       "  'accuracy': 0.9666666666666667,\n",
       "  'macro avg': {'precision': 0.9722222222222222,\n",
       "   'recall': 0.9629629629629629,\n",
       "   'f1-score': 0.9658994032395567,\n",
       "   'support': 30},\n",
       "  'weighted avg': {'precision': 0.9694444444444444,\n",
       "   'recall': 0.9666666666666667,\n",
       "   'f1-score': 0.9664109121909632,\n",
       "   'support': 30}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = train(X_path=prepare_result[\"X_path\"], y_path=prepare_result[\"y_path\"], output_dir=pipeline_run_id, clf_params={\"max_depth\": 2})\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5cda8e1b-a94c-49dc-bc3a-d5e8a43f2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(classification_report: dict, macro_avg_f1_score_min: float) -> dict:\n",
    "    params = locals()\n",
    "    from loguru import logger\n",
    "    logger.info(\"validate started.\")\n",
    "    macro_avg_f1_score = classification_report[\"macro avg\"][\"f1-score\"] \n",
    "    \n",
    "    if macro_avg_f1_score < macro_avg_f1_score_min:\n",
    "        passed = False\n",
    "    else: \n",
    "        passed = True\n",
    "    logger.info(\"validate finished.\")\n",
    "    return {\n",
    "        \"passed\": passed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c108d0b4-1301-4041-a231-0c567868fc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validate': 'passed'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_result = validate(classification_report=train_result[\"classification_report\"], macro_avg_f1_score_min=0.95)\n",
    "validate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af2985d5-9a01-4332-a573-32085849a6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validate': 'failed'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_result = validate(classification_report=train_result[\"classification_report\"], macro_avg_f1_score_min=0.99)\n",
    "validate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41531b2-0952-4dfa-9674-a3971eb8182a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9335a08-21b1-4f81-9a0c-5eaae5de373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def pipeline(\n",
    "    output_dir: str,\n",
    "    prepare_features: list,\n",
    "    prepare_standardize: bool,\n",
    "    train_clf_params: dict,\n",
    "    validate_macro_avg_f1_score_min: float\n",
    "):\n",
    "    if not output_dir:\n",
    "        output_dir = str(uuid.uuid4())\n",
    "    extract_result = extract(output_dir=output_dir)\n",
    "    \n",
    "    # TODO: json.dumps\n",
    "    prepare_result = prepare(raw_data_path=extract_result[\"raw_data_path\"], features=prepare_features, standardize=prepare_standardize, output_dir=output_dir)\n",
    "    train_result = train_result = train(X_path=prepare_result[\"X_path\"], y_path=prepare_result[\"y_path\"], output_dir=output_dir, clf_params=train_clf_params)\n",
    "    validate_result = validate(classification_report=train_result[\"classification_report\"], macro_avg_f1_score_min=validate_macro_avg_f1_score_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b83c6087-b2bd-43cc-9516-75da15c1d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd51a44a-5a6f-4619-967a-4c406904bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': None,\n",
       " 'prepare': {'features': ['sepal_length',\n",
       "   'sepal_width',\n",
       "   'petal_length',\n",
       "   'petal_width'],\n",
       "  'standardize': False},\n",
       " 'train': {'clf_params': {'criterion': 'gini', 'max_depth': 2}},\n",
       " 'validate': {'macro_avg_f1_score_min': 0.95}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config/run1.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "735f73f7-f88a-4c19-ad9e-19bd0395215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 00:57:18.385 | INFO     | __main__:extract:7 - extract started with {'output_dir': '57494766-001f-4e2b-86cd-d9d9e8685f54'}.\n",
      "2021-06-17 00:57:18.524 | INFO     | __main__:extract:20 - extract finished.\n",
      "2021-06-17 00:57:18.525 | INFO     | __main__:prepare:7 - prepare started with {'raw_data_path': '57494766-001f-4e2b-86cd-d9d9e8685f54/extract/raw_data.parquet', 'features': ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], 'standardize': False, 'output_dir': '57494766-001f-4e2b-86cd-d9d9e8685f54'}\n",
      "2021-06-17 00:57:18.533 | INFO     | __main__:prepare:28 - prepare finished.\n",
      "2021-06-17 00:57:18.533 | INFO     | __main__:train:10 - train started with {'X_path': '57494766-001f-4e2b-86cd-d9d9e8685f54/prepare/X.parquet', 'y_path': '57494766-001f-4e2b-86cd-d9d9e8685f54/prepare/y.parquet', 'output_dir': '57494766-001f-4e2b-86cd-d9d9e8685f54', 'clf_params': {'criterion': 'gini', 'max_depth': 2}}.\n",
      "2021-06-17 00:57:18.546 | INFO     | __main__:train:25 - train finished.\n",
      "2021-06-17 00:57:18.547 | INFO     | __main__:validate:4 - validate started.\n",
      "2021-06-17 00:57:18.547 | INFO     | __main__:validate:11 - validate finished.\n"
     ]
    }
   ],
   "source": [
    "pipeline(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    prepare_features=config[\"prepare\"][\"features\"],\n",
    "    prepare_standardize=config[\"prepare\"][\"standardize\"],\n",
    "    train_clf_params=config[\"train\"][\"clf_params\"],\n",
    "    validate_macro_avg_f1_score_min=config[\"validate\"][\"macro_avg_f1_score_min\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c52a14-25c8-4f0b-bd38-dfc73d79a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/run2.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7adb79f2-9387-4369-9388-cd1cb0d6d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 00:57:21.838 | INFO     | __main__:extract:7 - extract started with {'output_dir': 'ba941bc9-5877-4be0-b94d-e5d17f77ddb3'}.\n",
      "2021-06-17 00:57:22.010 | INFO     | __main__:extract:20 - extract finished.\n",
      "2021-06-17 00:57:22.011 | INFO     | __main__:prepare:7 - prepare started with {'raw_data_path': 'ba941bc9-5877-4be0-b94d-e5d17f77ddb3/extract/raw_data.parquet', 'features': ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], 'standardize': False, 'output_dir': 'ba941bc9-5877-4be0-b94d-e5d17f77ddb3'}\n",
      "2021-06-17 00:57:22.021 | INFO     | __main__:prepare:28 - prepare finished.\n",
      "2021-06-17 00:57:22.022 | INFO     | __main__:train:10 - train started with {'X_path': 'ba941bc9-5877-4be0-b94d-e5d17f77ddb3/prepare/X.parquet', 'y_path': 'ba941bc9-5877-4be0-b94d-e5d17f77ddb3/prepare/y.parquet', 'output_dir': 'ba941bc9-5877-4be0-b94d-e5d17f77ddb3', 'clf_params': {'criterion': 'gini', 'max_depth': 2}}.\n",
      "2021-06-17 00:57:22.036 | INFO     | __main__:train:25 - train finished.\n",
      "2021-06-17 00:57:22.037 | INFO     | __main__:validate:4 - validate started.\n",
      "2021-06-17 00:57:22.038 | INFO     | __main__:validate:11 - validate finished.\n"
     ]
    }
   ],
   "source": [
    "pipeline(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    prepare_features=config[\"prepare\"][\"features\"],\n",
    "    prepare_standardize=config[\"prepare\"][\"standardize\"],\n",
    "    train_clf_params=config[\"train\"][\"clf_params\"],\n",
    "    validate_macro_avg_f1_score_min=config[\"validate\"][\"macro_avg_f1_score_min\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43442789-2bea-4dda-8be8-98637e6c08a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
